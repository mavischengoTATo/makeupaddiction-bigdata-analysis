{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing large datasets with Apache Spark and Amazon SageMaker\n",
    "\n",
    "***This notebook run on `Data Science 3.0 - Python 3` kernel on a `ml.t3.large` instance***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker Processing Jobs are used  to analyze data and evaluate machine learning models on Amazon SageMaker. With Processing, you can use a simplified, managed experience on SageMaker to run your data processing workloads, such as feature engineering, data validation, model evaluation, and model interpretation. You can also use the Amazon SageMaker Processing APIs during the experimentation phase and after the code is deployed in production to evaluate performance.\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://docs.aws.amazon.com/images/sagemaker/latest/dg/images/Processing-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding diagram shows how Amazon SageMaker spins up a Processing job. Amazon SageMaker takes your script, copies your data from Amazon Simple Storage Service (Amazon S3), and then pulls a processing container. The processing container image can either be an Amazon SageMaker built-in image or a custom image that you provide. The underlying infrastructure for a Processing job is fully managed by Amazon SageMaker. Cluster resources are provisioned for the duration of your job, and cleaned up when a job completes. The output of the Processing job is stored in the Amazon S3 bucket you specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our workflow for processing large amounts of data with SageMaker\n",
    "\n",
    "We can divide our workflow into two steps:\n",
    "    \n",
    "1. Work with a small subset of the data with Spark running in local model in a SageMaker Studio Notebook.\n",
    "\n",
    "1. Once we are able to work with the small subset of data we can provide the same code (as a Python script rather than a series of interactive steps) to SageMaker Processing which launched a Spark cluster, runs out code and terminates the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this notebook...\n",
    "\n",
    "We will analyze the [Pushshift Reddit dataset](https://arxiv.org/pdf/2001.08435.pdf) to be used for the project and then we will run a SageMaker Processing Job to filter out the comments and submissions from subreddits of interest. The filtered data will be stored in your account's s3 bucket and it is this filtered data that you will be using for your project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We need an available Java installation to run pyspark. The easiest way to do this is to install JDK and set the proper paths using conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.3.1\n",
      "  latest version: 23.11.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=23.11.0\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pyspark==3.2.0 in /opt/conda/lib/python3.10/site-packages (3.2.0)\n",
      "Requirement already satisfied: py4j==0.10.9.2 in /opt/conda/lib/python3.10/site-packages (from pyspark==3.2.0) (0.10.9.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup - Run only once per Kernel App\n",
    "%conda install openjdk -y\n",
    "\n",
    "# install PySpark\n",
    "%pip install pyspark==3.2.0\n",
    "\n",
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.aws.credentials.provider\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/conda/lib/python3.10/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-65923ea2-8789-48f4-8cb4-8507f25e88ff;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.2.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.563 in central\n",
      ":: resolution report :: resolve 352ms :: artifacts dl 17ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.563 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.2.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-65923ea2-8789-48f4-8cb4-8507f25e88ff\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/9ms)\n",
      "23/12/05 05:27:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/05 05:27:18 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2.0\n"
     ]
    }
   ],
   "source": [
    "# Import pyspark and build Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"PySparkApp\")\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.2.2\")\n",
    "    .config(\n",
    "        \"fs.s3a.aws.credentials.provider\",\n",
    "        \"com.amazonaws.auth.ContainerCredentialsProvider\",\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process S3 data with SageMaker Processing Job `PySparkProcessor`\n",
    "\n",
    "We are going to move the above processing code in a Python file and then submit that file to SageMaker Processing Job's [`PySparkProcessor`](https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_processing.html#pysparkprocessor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p ./code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./code/process.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./code/process.py\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "# Import pyspark and build Spark session\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import (\n",
    "    DoubleType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    ")\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s,%(levelname)s,%(module)s,%(filename)s,%(lineno)d,%(message)s', level=logging.DEBUG)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"app inputs and outputs\")\n",
    "    parser.add_argument(\"--s3_dataset_path\", type=str, help=\"Path of dataset in S3\")    \n",
    "    parser.add_argument(\"--s3_output_bucket\", type=str, help=\"s3 output bucket\")\n",
    "    parser.add_argument(\"--s3_output_prefix\", type=str, help=\"s3 output prefix\")\n",
    "    parser.add_argument(\"--col_name_for_filtering\", type=str, help=\"Name of the column to filter\")\n",
    "    parser.add_argument(\"--values_to_keep\", type=str, help=\"comma separated list of values to keep in the filtered set\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    spark = SparkSession.builder.appName(\"PySparkApp\").getOrCreate()\n",
    "    logger.info(f\"spark version = {spark.version}\")\n",
    "    \n",
    "    # This is needed to save RDDs which is the only way to write nested Dataframes into CSV format\n",
    "    sc = spark.sparkContext\n",
    "    sc._jsc.hadoopConfiguration().set(\n",
    "        \"mapred.output.committer.class\", \"org.apache.hadoop.mapred.FileOutputCommitter\"\n",
    "    )\n",
    "\n",
    "   \n",
    "    # Downloading the data from S3 into a Dataframe\n",
    "    logger.info(f\"going to read {args.s3_dataset_path}\")\n",
    "    df = spark.read.parquet(args.s3_dataset_path, header=True)\n",
    "    logger.info(f\"finished reading files...\")\n",
    "    \n",
    "\n",
    "    \n",
    "    # filter the dataframe to only keep the values of interest\n",
    "    vals = [s.strip() for s in args.values_to_keep.split(\",\")]\n",
    "    df_filtered = df.where(col(args.col_name_for_filtering).isin(vals))\n",
    "    \n",
    "    # save the filtered dataframes so that these files can now be used for future analysis\n",
    "    s3_path = f\"s3://{args.s3_output_bucket}/{args.s3_output_prefix}\"\n",
    "    logger.info(f\"going to write data for {vals} in {s3_path}\")\n",
    "    logger.info(f\"shape of the df_filtered dataframe is {df_filtered.count():,}x{len(df_filtered.columns)}\")\n",
    "    df_filtered.write.mode(\"overwrite\").parquet(s3_path)\n",
    "    \n",
    "    logger.info(f\"all done...\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now submit this code to SageMaker Processing Job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "CPU times: user 2.68 s, sys: 340 ms, total: 3.02 s\n",
      "Wall time: 3.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "import sagemaker\n",
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "# Setup the PySpark processor to run the job. Note the instance type and instance count parameters. SageMaker will create these many instances of this type for the spark job.\n",
    "role = sagemaker.get_execution_role()\n",
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=\"sm-spark-project\",\n",
    "    framework_version=\"3.3\",\n",
    "    role=role,\n",
    "    instance_count=8,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=3600,\n",
    ")\n",
    "\n",
    "# s3 paths\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "output_prefix_logs = f\"spark_logs\"\n",
    "\n",
    "# modify this comma separated list to choose the subreddits of interest\n",
    "subreddits = \"MakeupAddiction\"\n",
    "configuration = [\n",
    "    {\n",
    "        \"Classification\": \"spark-defaults\",\n",
    "        \"Properties\": {\"spark.executor.memory\": \"12g\", \"spark.executor.cores\": \"4\"},\n",
    "    }\n",
    "]\n",
    "\n",
    "# the dataset contains data for these 3 years\n",
    "year_list = [2021,2022,2023]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to filter comments data for year=2021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name sm-spark-project-2023-12-05-05-27-25-071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........................................................................................................................................................................................................!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name sm-spark-project-2023-12-05-05-45-23-990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to filter comments data for year=2022\n",
      "......................................................................................................................................................................................................................!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name sm-spark-project-2023-12-05-06-04-27-827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to filter comments data for year=2023\n",
      ".......................................................................................................................!CPU times: user 2.52 s, sys: 379 ms, total: 2.9 s\n",
      "Wall time: 48min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for yyyy in year_list:\n",
    "    print(f\"going to filter comments data for year={yyyy}\")\n",
    "    s3_dataset_path_commments = f\"s3://bigdatateaching/reddit-parquet/comments/year={yyyy}/month=*/*.parquet\" # \"s3a://bigdatateaching/reddit/parquet/comments/yyyy=*/mm=*/*comments*.parquet\"\n",
    "    output_prefix_data_comments = f\"project/comments/yyyy={yyyy}\"\n",
    "    col_name_for_filtering = \"subreddit\"\n",
    "    subreddits = \"MakeupAddiction\"\n",
    "\n",
    "    # run the job now, the arguments array is provided as command line to the Python script (Spark code in this case).\n",
    "    spark_processor.run(\n",
    "        submit_app=\"./code/process.py\",\n",
    "        arguments=[\n",
    "            \"--s3_dataset_path\",\n",
    "            s3_dataset_path_commments,\n",
    "            \"--s3_output_bucket\",\n",
    "            bucket,\n",
    "            \"--s3_output_prefix\",\n",
    "            output_prefix_data_comments,\n",
    "            \"--col_name_for_filtering\",\n",
    "            col_name_for_filtering,\n",
    "            \"--values_to_keep\",\n",
    "            subreddits,\n",
    "        ],\n",
    "        spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(bucket, output_prefix_logs),\n",
    "        logs=False,\n",
    "        configuration=configuration\n",
    "    )\n",
    "    # give some time for resources from this iterations to get cleaned up\n",
    "    # if we start the job immediately we could get insufficient resources error\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "year_list = [2021,2022,2023]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to filter submissions data for year=2021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name sm-spark-project-2023-12-05-06-15-33-523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................................................................................................................................................................................................................................................!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name sm-spark-project-2023-12-05-06-37-03-432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to filter submissions data for year=2022\n",
      "...........................................................................................................................................................................................................................................................!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name sm-spark-project-2023-12-05-06-59-13-642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to filter submissions data for year=2023\n",
      "................................................................................................................................................!CPU times: user 2.9 s, sys: 459 ms, total: 3.36 s\n",
      "Wall time: 56min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for yyyy in year_list:\n",
    "    print(f\"going to filter submissions data for year={yyyy}\")\n",
    "    s3_dataset_path_submissions = f\"s3://bigdatateaching/reddit-parquet/submissions/year={yyyy}/month=*/*.parquet\" # \"s3a://bigdatateaching/reddit/parquet/submissions/yyyy=*/mm=*/*submissions*.parquet\"\n",
    "    output_prefix_data_submissions = f\"project/submissions/yyyy={yyyy}\"\n",
    "\n",
    "    col_name_for_filtering = \"subreddit\"\n",
    "    subreddits = \"MakeupAddiction\"\n",
    "    # run the job now, the arguments array is provided as command line to the Python script (Spark code in this case).\n",
    "    spark_processor.run(\n",
    "        submit_app=\"./code/process.py\",\n",
    "        arguments=[\n",
    "             \"--s3_dataset_path\",\n",
    "            s3_dataset_path_submissions,\n",
    "            \"--s3_output_bucket\",\n",
    "            bucket,\n",
    "            \"--s3_output_prefix\",\n",
    "            output_prefix_data_submissions,\n",
    "            \"--col_name_for_filtering\",\n",
    "            col_name_for_filtering,\n",
    "            \"--values_to_keep\",\n",
    "            subreddits,\n",
    "        ],\n",
    "        spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(bucket, output_prefix_logs),\n",
    "        logs=False,\n",
    "        configuration=configuration\n",
    "    )\n",
    "    # give some time for resources from this iterations to get cleaned up\n",
    "    # if we start the job immediately we could get insufficient resources error\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the filtered data\n",
    "\n",
    "Now that we have filtered the data to only keep submissions and comments from subreddits of interest. Let us read data from the s3 path where we saved the filtered data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "reading comments from s3a://sagemaker-us-east-1-834346005548/project/comments/yyyy=*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:=====================================================>(240 + 1) / 241]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the comments dataframe is 1,053,280x21\n",
      "CPU times: user 353 ms, sys: 52.7 ms, total: 406 ms\n",
      "Wall time: 5min 20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import sagemaker\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "output_prefix_data_comments = \"project/comments/yyyy=*\"\n",
    "s3_path = f\"s3a://{bucket}/{output_prefix_data_comments}\"\n",
    "#s3_path = \"s3a://sagemaker-us-east-1-038932893404/project/comments/yyyy=2021/part-00000-90796409-5783-4705-92c0-27c27eda8c4c-c000.snappy.parquet\"\n",
    "print(f\"reading comments from {s3_path}\")\n",
    "comments = spark.read.parquet(s3_path, header=True)\n",
    "print(f\"shape of the comments dataframe is {comments.count():,}x{len(comments.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:=====================================================>(240 + 1) / 241]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+\n",
      "|      subreddit|  count|\n",
      "+---------------+-------+\n",
      "|MakeupAddiction|1053280|\n",
      "+---------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# check counts (ensuring all needed subreddits exist)\n",
    "comments.groupBy('subreddit').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+--------------------+----------+---------+-------+-------------------+\n",
      "|      subreddit|              author|                body| parent_id|  link_id|     id|        created_utc|\n",
      "+---------------+--------------------+--------------------+----------+---------+-------+-------------------+\n",
      "|MakeupAddiction|        chinghiskhan|  Aww, thank you! 💙|t1_hs5p1jj|t3_s125dy|hs5vt5h|2022-01-11 04:04:19|\n",
      "|MakeupAddiction|        chinghiskhan|Aw thanks, that’s...|t1_hs5q3g2|t3_s125dy|hs5vv80|2022-01-11 04:04:46|\n",
      "|MakeupAddiction|        chinghiskhan|Omg thank you so ...|t1_hs5r131|t3_s125dy|hs5vwrf|2022-01-11 04:05:04|\n",
      "|MakeupAddiction|        ssuperficial|Too Faced had a k...| t3_s0u3nh|t3_s0u3nh|hs5wdjb|2022-01-11 04:08:26|\n",
      "|MakeupAddiction|Equivalent-Newt-4592|All the eyeshadow...|t1_hs5p5dn|t3_s11hyr|hs5wh9u|2022-01-11 04:09:07|\n",
      "|MakeupAddiction|           ceklassen|Beautiful colours...| t3_s125dy|t3_s125dy|hs5wkwp|2022-01-11 04:09:46|\n",
      "|MakeupAddiction|        chinghiskhan|So beautiful!! I ...| t3_s11hyr|t3_s11hyr|hs5wlf5|2022-01-11 04:09:52|\n",
      "|MakeupAddiction|           ceklassen|Love that shade o...| t3_s0zxxu|t3_s0zxxu|hs5wt4e|2022-01-11 04:11:23|\n",
      "|MakeupAddiction|           [deleted]|           [deleted]| t3_s0zxxu|t3_s0zxxu|hs5wxlb|2022-01-11 04:12:16|\n",
      "|MakeupAddiction|          lykaromazi|Thank you so much...|t1_hs1qnoc|t3_s009zq|hs5x123|2022-01-11 04:12:58|\n",
      "|MakeupAddiction|       IGotOverGreta|            So good!| t3_s125dy|t3_s125dy|hs5x1jc|2022-01-11 04:13:03|\n",
      "|MakeupAddiction|           kcelaynes|  Lips look so good!| t3_s0zxxu|t3_s0zxxu|hs5xd6i|2022-01-11 04:15:20|\n",
      "|MakeupAddiction|           [deleted]|           [deleted]| t3_s0zxxu|t3_s0zxxu|hs5xe6m|2022-01-11 04:15:31|\n",
      "|MakeupAddiction|       eatstressbake|Some people swear...|t1_hs2wg3d|t3_s0k0ac|hs5xtq0|2022-01-11 04:18:38|\n",
      "|MakeupAddiction|      Raven_Ironwing|This is largely w...|t1_i1c5erd|t3_ti3poz|i1ch8n3|2022-03-20 00:22:32|\n",
      "|MakeupAddiction|       This-Air-9328|Use mac prep and ...| t3_ti2gbd|t3_ti2gbd|i1chhqj|2022-03-20 00:24:36|\n",
      "|MakeupAddiction|  InternalBobcat4443|          Love it!!!| t3_ti2y7u|t3_ti2y7u|i1chizc|2022-03-20 00:24:53|\n",
      "|MakeupAddiction|       AutoModerator|***Thank you for ...| t3_ti8xjn|t3_ti8xjn|i1chrrq|2022-03-20 00:26:51|\n",
      "|MakeupAddiction|         Lexy_d_acnh|Mascara would loo...| t3_thyvn0|t3_thyvn0|i1chvaj|2022-03-20 00:27:41|\n",
      "|MakeupAddiction|          sqwidsqwad|Thr feathered loo...| t3_thwfq2|t3_thwfq2|i1chwgq|2022-03-20 00:27:57|\n",
      "+---------------+--------------------+--------------------+----------+---------+-------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display a subset of columns\n",
    "comments.select(\"subreddit\", \"author\", \"body\", \"parent_id\", \"link_id\", \"id\", \"created_utc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- author_cakeday: boolean (nullable = true)\n",
      " |-- author_flair_css_class: string (nullable = true)\n",
      " |-- author_flair_text: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- can_gild: boolean (nullable = true)\n",
      " |-- controversiality: long (nullable = true)\n",
      " |-- created_utc: timestamp (nullable = true)\n",
      " |-- distinguished: string (nullable = true)\n",
      " |-- edited: string (nullable = true)\n",
      " |-- gilded: long (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- is_submitter: boolean (nullable = true)\n",
      " |-- link_id: string (nullable = true)\n",
      " |-- parent_id: string (nullable = true)\n",
      " |-- permalink: string (nullable = true)\n",
      " |-- retrieved_on: timestamp (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- stickied: boolean (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- subreddit_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comments.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample the dataset around 10000 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 1053280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique authors: 193808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:=====================================================>(240 + 1) / 241]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique subreddits: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of rows: {comments.count()}\")\n",
    "print(f\"Total number of unique authors: {comments.select('author').distinct().count()}\")\n",
    "print(f\"Total number of unique subreddits: {comments.select('subreddit').distinct().count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, length\n",
    "# Check for missing values in all columns\n",
    "def check_missing_values(df):\n",
    "    for column in df.columns:\n",
    "        missing_values = df.filter(col(column).isNull()).count()\n",
    "        if missing_values > 0:\n",
    "            print(f\"Column {column} has {missing_values} missing values\")\n",
    "        else:\n",
    "            print(f\"Column {column} has no missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for rows with very short comments, which may not be useful for analysis\n",
    "def filter_short_comments(df, min_length=10):\n",
    "    return df.filter(length(col(\"comment\")) >= min_length)\n",
    "\n",
    "# Remove corrupted rows, here we define 'corrupted' rows as examples, it can include any row that does not meet your data criteria\n",
    "def remove_corrupted_rows(df):\n",
    "    # we remove rows where 'user_id' is negative or null as an example\n",
    "    df = df.filter(col(\"user_id\").isNotNull() & (col(\"user_id\") > 0))\n",
    "    # Add more filters based on your data and needs\n",
    "    return df\n",
    "\n",
    "# Apply the functions to check and clean the dataset\n",
    "def clean_data(df):\n",
    "    check_missing_values(df)\n",
    "    df = filter_short_comments(df)\n",
    "    df = remove_corrupted_rows(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column author has no missing values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:==================>                                    (81 + 2) / 241]\r"
     ]
    }
   ],
   "source": [
    "cleaned_sampled_comments = clean_data(sampled_comments)\n",
    "\n",
    "# Count the number of rows after cleaning the data\n",
    "row_count_after_cleaning = cleaned_sampled_comments.count()\n",
    "print(f\"Number of rows after cleaning: {row_count_after_cleaning}\")\n",
    "\n",
    "# If you want to show the DataFrame\n",
    "#cleaned_sampled_comments.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "reading submissions from s3a://sagemaker-us-east-1-834346005548/project/submissions/yyyy=*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:=======================================================>(96 + 1) / 97]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the submissions dataframe is 95,932x68\n",
      "CPU times: user 224 ms, sys: 15.6 ms, total: 239 ms\n",
      "Wall time: 2min 49s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import sagemaker\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "output_prefix_data_submissions = f\"project/submissions/yyyy=*\"\n",
    "s3_path = f\"s3a://{bucket}/{output_prefix_data_submissions}\"\n",
    "print(f\"reading submissions from {s3_path}\")\n",
    "submissions = spark.read.parquet(s3_path, header=True)\n",
    "print(f\"shape of the submissions dataframe is {submissions.count():,}x{len(submissions.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 43:=======================================================>(96 + 1) / 97]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|      subreddit|count|\n",
      "+---------------+-----+\n",
      "|MakeupAddiction|95932|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# check counts (ensuring all needed subreddits exist)\n",
    "submissions.groupBy('subreddit').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- adserver_click_url: string (nullable = true)\n",
      " |-- adserver_imp_pixel: string (nullable = true)\n",
      " |-- archived: boolean (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- author_cakeday: boolean (nullable = true)\n",
      " |-- author_flair_css_class: string (nullable = true)\n",
      " |-- author_flair_text: string (nullable = true)\n",
      " |-- author_id: string (nullable = true)\n",
      " |-- brand_safe: boolean (nullable = true)\n",
      " |-- contest_mode: boolean (nullable = true)\n",
      " |-- created_utc: timestamp (nullable = true)\n",
      " |-- crosspost_parent: string (nullable = true)\n",
      " |-- crosspost_parent_list: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- approved_at_utc: string (nullable = true)\n",
      " |    |    |-- approved_by: string (nullable = true)\n",
      " |    |    |-- archived: boolean (nullable = true)\n",
      " |    |    |-- author: string (nullable = true)\n",
      " |    |    |-- author_flair_css_class: string (nullable = true)\n",
      " |    |    |-- author_flair_text: string (nullable = true)\n",
      " |    |    |-- banned_at_utc: string (nullable = true)\n",
      " |    |    |-- banned_by: string (nullable = true)\n",
      " |    |    |-- brand_safe: boolean (nullable = true)\n",
      " |    |    |-- can_gild: boolean (nullable = true)\n",
      " |    |    |-- can_mod_post: boolean (nullable = true)\n",
      " |    |    |-- clicked: boolean (nullable = true)\n",
      " |    |    |-- contest_mode: boolean (nullable = true)\n",
      " |    |    |-- created: double (nullable = true)\n",
      " |    |    |-- created_utc: double (nullable = true)\n",
      " |    |    |-- distinguished: string (nullable = true)\n",
      " |    |    |-- domain: string (nullable = true)\n",
      " |    |    |-- downs: long (nullable = true)\n",
      " |    |    |-- edited: boolean (nullable = true)\n",
      " |    |    |-- gilded: long (nullable = true)\n",
      " |    |    |-- hidden: boolean (nullable = true)\n",
      " |    |    |-- hide_score: boolean (nullable = true)\n",
      " |    |    |-- id: string (nullable = true)\n",
      " |    |    |-- is_crosspostable: boolean (nullable = true)\n",
      " |    |    |-- is_reddit_media_domain: boolean (nullable = true)\n",
      " |    |    |-- is_self: boolean (nullable = true)\n",
      " |    |    |-- is_video: boolean (nullable = true)\n",
      " |    |    |-- likes: string (nullable = true)\n",
      " |    |    |-- link_flair_css_class: string (nullable = true)\n",
      " |    |    |-- link_flair_text: string (nullable = true)\n",
      " |    |    |-- locked: boolean (nullable = true)\n",
      " |    |    |-- media: string (nullable = true)\n",
      " |    |    |-- mod_reports: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- num_comments: long (nullable = true)\n",
      " |    |    |-- num_crossposts: long (nullable = true)\n",
      " |    |    |-- num_reports: string (nullable = true)\n",
      " |    |    |-- over_18: boolean (nullable = true)\n",
      " |    |    |-- parent_whitelist_status: string (nullable = true)\n",
      " |    |    |-- permalink: string (nullable = true)\n",
      " |    |    |-- pinned: boolean (nullable = true)\n",
      " |    |    |-- quarantine: boolean (nullable = true)\n",
      " |    |    |-- removal_reason: string (nullable = true)\n",
      " |    |    |-- report_reasons: string (nullable = true)\n",
      " |    |    |-- saved: boolean (nullable = true)\n",
      " |    |    |-- score: long (nullable = true)\n",
      " |    |    |-- secure_media: string (nullable = true)\n",
      " |    |    |-- selftext: string (nullable = true)\n",
      " |    |    |-- selftext_html: string (nullable = true)\n",
      " |    |    |-- spoiler: boolean (nullable = true)\n",
      " |    |    |-- stickied: boolean (nullable = true)\n",
      " |    |    |-- subreddit: string (nullable = true)\n",
      " |    |    |-- subreddit_id: string (nullable = true)\n",
      " |    |    |-- subreddit_name_prefixed: string (nullable = true)\n",
      " |    |    |-- subreddit_type: string (nullable = true)\n",
      " |    |    |-- suggested_sort: string (nullable = true)\n",
      " |    |    |-- thumbnail: string (nullable = true)\n",
      " |    |    |-- thumbnail_height: string (nullable = true)\n",
      " |    |    |-- thumbnail_width: string (nullable = true)\n",
      " |    |    |-- title: string (nullable = true)\n",
      " |    |    |-- ups: long (nullable = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- user_reports: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- view_count: string (nullable = true)\n",
      " |    |    |-- visited: boolean (nullable = true)\n",
      " |    |    |-- whitelist_status: string (nullable = true)\n",
      " |-- disable_comments: boolean (nullable = true)\n",
      " |-- distinguished: string (nullable = true)\n",
      " |-- domain: string (nullable = true)\n",
      " |-- domain_override: string (nullable = true)\n",
      " |-- edited: string (nullable = true)\n",
      " |-- embed_type: string (nullable = true)\n",
      " |-- embed_url: string (nullable = true)\n",
      " |-- gilded: long (nullable = true)\n",
      " |-- hidden: boolean (nullable = true)\n",
      " |-- hide_score: boolean (nullable = true)\n",
      " |-- href_url: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- imp_pixel: string (nullable = true)\n",
      " |-- is_crosspostable: boolean (nullable = true)\n",
      " |-- is_reddit_media_domain: boolean (nullable = true)\n",
      " |-- is_self: boolean (nullable = true)\n",
      " |-- is_video: boolean (nullable = true)\n",
      " |-- link_flair_css_class: string (nullable = true)\n",
      " |-- link_flair_text: string (nullable = true)\n",
      " |-- locked: boolean (nullable = true)\n",
      " |-- media: struct (nullable = true)\n",
      " |    |-- event_id: string (nullable = true)\n",
      " |    |-- oembed: struct (nullable = true)\n",
      " |    |    |-- author_name: string (nullable = true)\n",
      " |    |    |-- author_url: string (nullable = true)\n",
      " |    |    |-- cache_age: long (nullable = true)\n",
      " |    |    |-- description: string (nullable = true)\n",
      " |    |    |-- height: long (nullable = true)\n",
      " |    |    |-- html: string (nullable = true)\n",
      " |    |    |-- provider_name: string (nullable = true)\n",
      " |    |    |-- provider_url: string (nullable = true)\n",
      " |    |    |-- thumbnail_height: long (nullable = true)\n",
      " |    |    |-- thumbnail_url: string (nullable = true)\n",
      " |    |    |-- thumbnail_width: long (nullable = true)\n",
      " |    |    |-- title: string (nullable = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- version: string (nullable = true)\n",
      " |    |    |-- width: long (nullable = true)\n",
      " |    |-- reddit_video: struct (nullable = true)\n",
      " |    |    |-- dash_url: string (nullable = true)\n",
      " |    |    |-- duration: long (nullable = true)\n",
      " |    |    |-- fallback_url: string (nullable = true)\n",
      " |    |    |-- height: long (nullable = true)\n",
      " |    |    |-- hls_url: string (nullable = true)\n",
      " |    |    |-- is_gif: boolean (nullable = true)\n",
      " |    |    |-- scrubber_media_url: string (nullable = true)\n",
      " |    |    |-- transcoding_status: string (nullable = true)\n",
      " |    |    |-- width: long (nullable = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |-- media_embed: struct (nullable = true)\n",
      " |    |-- content: string (nullable = true)\n",
      " |    |-- height: long (nullable = true)\n",
      " |    |-- scrolling: boolean (nullable = true)\n",
      " |    |-- width: long (nullable = true)\n",
      " |-- mobile_ad_url: string (nullable = true)\n",
      " |-- num_comments: long (nullable = true)\n",
      " |-- num_crossposts: long (nullable = true)\n",
      " |-- original_link: string (nullable = true)\n",
      " |-- over_18: boolean (nullable = true)\n",
      " |-- parent_whitelist_status: string (nullable = true)\n",
      " |-- permalink: string (nullable = true)\n",
      " |-- pinned: boolean (nullable = true)\n",
      " |-- post_hint: string (nullable = true)\n",
      " |-- preview: struct (nullable = true)\n",
      " |    |-- enabled: boolean (nullable = true)\n",
      " |    |-- images: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- id: string (nullable = true)\n",
      " |    |    |    |-- resolutions: array (nullable = true)\n",
      " |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |-- height: long (nullable = true)\n",
      " |    |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |    |    |-- width: long (nullable = true)\n",
      " |    |    |    |-- source: struct (nullable = true)\n",
      " |    |    |    |    |-- height: long (nullable = true)\n",
      " |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |    |-- width: long (nullable = true)\n",
      " |    |    |    |-- variants: struct (nullable = true)\n",
      " |    |    |    |    |-- gif: struct (nullable = true)\n",
      " |    |    |    |    |    |-- resolutions: array (nullable = true)\n",
      " |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |-- height: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- width: long (nullable = true)\n",
      " |    |    |    |    |    |-- source: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- height: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- width: long (nullable = true)\n",
      " |    |    |    |    |-- mp4: struct (nullable = true)\n",
      " |    |    |    |    |    |-- resolutions: array (nullable = true)\n",
      " |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |-- height: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- width: long (nullable = true)\n",
      " |    |    |    |    |    |-- source: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- height: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- width: long (nullable = true)\n",
      " |    |    |    |    |-- nsfw: struct (nullable = true)\n",
      " |    |    |    |    |    |-- resolutions: array (nullable = true)\n",
      " |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |-- height: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- width: long (nullable = true)\n",
      " |    |    |    |    |    |-- source: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- height: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- width: long (nullable = true)\n",
      " |    |    |    |    |-- obfuscated: struct (nullable = true)\n",
      " |    |    |    |    |    |-- resolutions: array (nullable = true)\n",
      " |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |-- height: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- width: long (nullable = true)\n",
      " |    |    |    |    |    |-- source: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- height: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- width: long (nullable = true)\n",
      " |-- promoted: boolean (nullable = true)\n",
      " |-- promoted_by: string (nullable = true)\n",
      " |-- promoted_display_name: string (nullable = true)\n",
      " |-- promoted_url: string (nullable = true)\n",
      " |-- retrieved_on: timestamp (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- secure_media: struct (nullable = true)\n",
      " |    |-- event_id: string (nullable = true)\n",
      " |    |-- oembed: struct (nullable = true)\n",
      " |    |    |-- author_name: string (nullable = true)\n",
      " |    |    |-- author_url: string (nullable = true)\n",
      " |    |    |-- cache_age: long (nullable = true)\n",
      " |    |    |-- description: string (nullable = true)\n",
      " |    |    |-- height: long (nullable = true)\n",
      " |    |    |-- html: string (nullable = true)\n",
      " |    |    |-- provider_name: string (nullable = true)\n",
      " |    |    |-- provider_url: string (nullable = true)\n",
      " |    |    |-- thumbnail_height: long (nullable = true)\n",
      " |    |    |-- thumbnail_url: string (nullable = true)\n",
      " |    |    |-- thumbnail_width: long (nullable = true)\n",
      " |    |    |-- title: string (nullable = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- version: string (nullable = true)\n",
      " |    |    |-- width: long (nullable = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |-- secure_media_embed: struct (nullable = true)\n",
      " |    |-- content: string (nullable = true)\n",
      " |    |-- height: long (nullable = true)\n",
      " |    |-- media_domain_url: string (nullable = true)\n",
      " |    |-- scrolling: boolean (nullable = true)\n",
      " |    |-- width: long (nullable = true)\n",
      " |-- selftext: string (nullable = true)\n",
      " |-- spoiler: boolean (nullable = true)\n",
      " |-- stickied: boolean (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- subreddit_id: string (nullable = true)\n",
      " |-- suggested_sort: string (nullable = true)\n",
      " |-- third_party_trackers: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- third_party_tracking: string (nullable = true)\n",
      " |-- third_party_tracking_2: string (nullable = true)\n",
      " |-- thumbnail: string (nullable = true)\n",
      " |-- thumbnail_height: long (nullable = true)\n",
      " |-- thumbnail_width: long (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- whitelist_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "submissions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+--------------------+--------------------+-------------------+------------+\n",
      "|      subreddit|              author|               title|            selftext|        created_utc|num_comments|\n",
      "+---------------+--------------------+--------------------+--------------------+-------------------+------------+\n",
      "|MakeupAddiction|  DefinitlyNotTheFBl|Boyfriend trying ...|                    |2023-01-18 23:43:31|          13|\n",
      "|MakeupAddiction|           [deleted]|i’m going to a pa...|           [removed]|2023-01-18 23:44:34|           7|\n",
      "|MakeupAddiction|            Readkt92|2023 Blush tracke...|                    |2023-01-18 23:46:25|           6|\n",
      "|MakeupAddiction|               NoheD|How would you con...|                    |2023-01-18 23:57:36|          12|\n",
      "|MakeupAddiction|      throwaway_2613|what can i do to ...|                    |2023-01-19 00:02:52|         147|\n",
      "|MakeupAddiction|HuckleberryNeither63|Brow lift product...|I tried out the e...|2023-01-25 22:59:17|           3|\n",
      "|MakeupAddiction|          dansdlopes|Morphe palettes -...|Now that the I've...|2023-01-25 23:10:51|           7|\n",
      "|MakeupAddiction|           [deleted]|My everyday makeu...|           [deleted]|2023-01-25 23:18:37|           2|\n",
      "|MakeupAddiction|Good-Temperature1737|  date night makeup!|                    |2023-01-25 23:26:00|           2|\n",
      "|MakeupAddiction|           mrsmynxxx|date night look, ...|                    |2023-01-25 23:27:09|           4|\n",
      "|MakeupAddiction|       Salty_Herring|Trying a slightly...|As always, any fe...|2023-01-25 23:27:41|           3|\n",
      "|MakeupAddiction|       carolairdluvr|Everyday makeup o...|                    |2023-01-25 23:28:45|          38|\n",
      "|MakeupAddiction|    QuitUsingMyNames|Me after trying a...|                    |2023-01-25 23:31:12|           3|\n",
      "|MakeupAddiction|      nochnoyvangogh|waterline eyeline...|hi! so as the tit...|2023-03-06 22:45:31|           2|\n",
      "|MakeupAddiction|     4and3and2and1_1|DIOR FOREVER SKIN...|           [removed]|2023-03-06 22:46:57|           1|\n",
      "|MakeupAddiction|     Blueberrysushii|I wanna start doi...|           [removed]|2023-03-06 22:49:00|           2|\n",
      "|MakeupAddiction|           Karenty83| Make up today! 👍🏻|                    |2023-03-06 22:49:57|           2|\n",
      "|MakeupAddiction|           [deleted]|Any makeup CC? Sh...|           [deleted]|2023-03-06 22:51:13|           2|\n",
      "|MakeupAddiction|      StridentTyrant|Could I use any p...|                    |2023-03-06 22:52:40|           1|\n",
      "|MakeupAddiction|     Blueberrysushii|I wanna begin my ...|Do you have any e...|2023-03-06 22:53:51|           4|\n",
      "+---------------+--------------------+--------------------+--------------------+-------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display a subset of columns\n",
    "submissions.select(\"subreddit\", \"author\", \"title\", \"selftext\", \"created_utc\", \"num_comments\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=======================================================> (95 + 2) / 97]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 95932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of rows: {submissions.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:=======================================================>(96 + 1) / 97]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique authors: 37635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of unique authors: {submissions.select('author').distinct().count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column adserver_click_url has 9734 missing values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column adserver_imp_pixel has 9734 missing values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:==========>                                             (18 + 2) / 97]\r"
     ]
    }
   ],
   "source": [
    "cleaned_submissions = clean_data(submissions)\n",
    "\n",
    "# Count the number of rows after cleaning the data\n",
    "row_count_after_cleaning_submissions = cleaned_submissions.count()\n",
    "print(f\"Number of rows after cleaning: {row_count_after_cleaning_submissions}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
